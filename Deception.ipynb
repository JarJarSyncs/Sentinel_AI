{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e75af8-fe96-4e87-b99c-861bd10e649e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2203f7b5-be5a-4386-82ca-bd9ab294dfe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAME_SIZE = (224, 224)       # Size of each video frame\n",
    "DECEPTION_CLASSES = 1         # Binary deception classification\n",
    "BATCH_SIZE = 1\n",
    "PAD_LENGTH = 600\n",
    "EPOCHS = 1               # Reduced for demonstration purposes\n",
    "NUM_FRAMES=16\n",
    "FRAME_CLUSTER = 1\n",
    "DIR_WIN = 'C:/Users/jettc/Downloads/Tony file2/'\n",
    "DIR_LIN = '/mnt/c/Users/jettc/Downloads/Tony file2/'\n",
    "VIDEO_DIR = DIR_LIN    # Directory where video files are stored\n",
    "LABEL_FILE_PATH = VIDEO_DIR+'Actions folder/Tony Gestures (Deceptive 10.9.23) .xlsx'\n",
    "\n",
    "# Columns corresponding to action labels in the dataset\n",
    "ACTION_COLUMNS = ['forewardHead', 'tiltHead', 'downHead', 'scanningHead', 'reflexHead', 'vigilantGaze', 'orientingGaze', \n",
    "                  'downGaze', 'otherGaze', 'scanningGaze', 'foldedArms', 'holdingArms', 'hidingArms', 'soothingArms', \n",
    "                  'otherArms', 'interlockedHand', 'holdingHands', 'hidingHand', 'soothinHandM', 'wringingHandM', 'distractingHands']\n",
    "\n",
    "ACTION_CLASSES = len(ACTION_COLUMNS)           # Number of action classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "12f8b323-1025-4fc7-8b48-691418b24c24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5b06630b-6fd6-4eaf-bab0-de473f22091e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/jettc/OneDrive - Swinburne University/5th Year/Semester 2/Computing Technology Project B'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f7905650-9454-4cea-9d7e-bbdfa76825ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labels(label_file_path):\n",
    "    \"\"\"Load labels for actions and deception from the Excel file.\"\"\"\n",
    "    df = pd.read_excel(label_file_path, sheet_name='Tony Gestures_Deceptive and Tru')\n",
    "    \n",
    "    \n",
    "    DECEPTION_COLUMN = 'class'\n",
    "    \n",
    "    video_files = []\n",
    "    action_labels = []\n",
    "    deception_labels = []\n",
    "    \n",
    "    df = df.dropna(subset=ACTION_COLUMNS)\n",
    "    \n",
    "    # Extract video paths and labels\n",
    "    for index, row in df.iterrows():\n",
    "        video_file = os.path.join(VIDEO_DIR, str(row['id']))\n",
    "        if not os.path.exists(video_file):\n",
    "            print(f\"Warning: Video file {video_file} not found!\")\n",
    "            continue\n",
    "        # Extract action labels\n",
    "        actions = row[ACTION_COLUMNS].values.astype(float)\n",
    "        #actions = np.tile(actions, (PAD_LENGTH, 1))\n",
    "        action_labels.append(actions)\n",
    "        \n",
    "        # Deception label\n",
    "        deception = 1 if 'deceptive' in str(row[DECEPTION_COLUMN]).lower().strip() else 0\n",
    "        deception_labels.append(deception)\n",
    "        video_files.append(video_file)\n",
    "    \n",
    "    return video_files, np.array(action_labels), np.array(deception_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eef2108e-93dc-406e-825f-8e44f4004dee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jettc/anaconda3/envs/py311/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos: 168\n",
      "Action labels shape: (168, 21)\n",
      "Deception labels shape: (168,)\n"
     ]
    }
   ],
   "source": [
    "# Load labels and video file paths\n",
    "video_files, action_labels, deception_labels = load_labels(LABEL_FILE_PATH)\n",
    "\n",
    "print(\"Number of videos:\", len(video_files))\n",
    "print(\"Action labels shape:\", action_labels.shape)\n",
    "print(\"Deception labels shape:\", deception_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "962774f3-74a9-4c9e-8bb8-ef2adba8c210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_every_nth_frame(video_path, n=NUM_FRAMES, target_size=FRAME_SIZE):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return None\n",
    "\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    while success:\n",
    "        # Only process every nth frame\n",
    "        if frame_count % n == 0:\n",
    "            # Resize the frame to reduce memory usage if necessary\n",
    "            if target_size:\n",
    "                frame = cv2.resize(frame, target_size)\n",
    "            frames.append(frame)\n",
    "\n",
    "        # Skip to the next frame\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return np.array(frames, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2c60b674-c465-406e-ad0c-7ad9c64d24f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "video_train, video_test, action_train, action_test, deception_train, deception_test = train_test_split(\n",
    "    video_files, action_labels, deception_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "del video_files, action_labels, deception_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "36b34639-e1c9-48a7-911b-43c4dec4bfac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pad all videos to the same length (PAD_LENGTH)\n",
    "def pad_videos(frames, maxlen=PAD_LENGTH):\n",
    "    # Get frame dimensions (height, width, channels)\n",
    "    frame_height, frame_width, channels = frames[0].shape\n",
    "    \n",
    "    # Initialize an array of zeros for padding (with shape of maxlen frames)\n",
    "    padded_frames = np.zeros((maxlen, frame_height, frame_width, channels), dtype=np.float32)\n",
    "    \n",
    "    # Determine how many frames to copy\n",
    "    num_frames = min(len(frames), maxlen)\n",
    "    \n",
    "    # Copy the original frames into the padded array\n",
    "    padded_frames[:num_frames] = frames[:num_frames]\n",
    "    \n",
    "    return padded_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314dce86-6707-4a26-80ee-01132ad1cc67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jettc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Vision Transformer model from TensorFlow Hub\n",
    "vit_layer = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b32_fe/1\", trainable=True)\n",
    "\n",
    "# Function to preprocess a batch of frames using the Vision Transformer\n",
    "def preprocess_frames_with_vit(frames_batch):\n",
    "    \"\"\"\n",
    "    Given a batch of frames, process each frame through the Vision Transformer to extract embeddings.\n",
    "    :param frames_batch: A numpy array of shape (NUM_FRAMES, 224, 224, 3)\n",
    "    :return: A numpy array of embeddings for each frame (NUM_FRAMES, embedding_dim)\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for frame in frames_batch:\n",
    "        frame = frame.astype('float32') / 255.0\n",
    "        \n",
    "        # Ensure each frame is processed individually\n",
    "        frame = np.expand_dims(frame, axis=0)  # Add batch dimension\n",
    "        embedding = vit_layer(frame)           # Get the ViT embedding for this frame\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Convert the list of embeddings to a NumPy array before checking the shape\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    print(f\"Shape of frame embeddings: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings  # Return the stacked embeddings as a sequence\n",
    "\n",
    "# Compute embedding_dim using a sample frame\n",
    "sample_frame = np.random.rand(1, 224, 224, 3).astype(np.float32)  # A random sample frame\n",
    "sample_embedding = vit_layer(sample_frame)\n",
    "embedding_dim = sample_embedding.shape[-1]  # Get the embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3c6ad28b-6f68-4d5d-9fb1-a4d418ce85ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape to the model: (1, 1, 768)\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM and Multi-task Learning Model\n",
    "def build_lstm_model(embedding_dim, batch_size=BATCH_SIZE):\n",
    "    inputs = layers.Input(batch_shape=(batch_size, FRAME_CLUSTER, embedding_dim))  # batch_shape for stateful LSTM\n",
    "    print(f\"Input shape to the model: {inputs.shape}\")\n",
    "    \n",
    "    # Masking to ignore padded frames\n",
    "    #masked_inputs = layers.Masking(mask_value=0.0)(inputs)  # Assumes padding value is 0\n",
    "    \n",
    "    # LSTM for temporal modeling across the frames\n",
    "    dense_action = layers.Dense(128, activation='relu')(inputs)\n",
    "    attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=128)(dense_action, dense_action)\n",
    "    dropout_action = layers.Dropout(0.3)(attention_output)\n",
    "\n",
    "\n",
    "    # Action detection: A TimeDistributed layer to classify actions for each frame\n",
    "    action_output = layers.Dense(ACTION_CLASSES, activation='sigmoid', name=\"action_output\")(dropout_action)\n",
    "    \n",
    "    # LSTM for deception classification (stateful)\n",
    "    lstm_deception = layers.LSTM(64, return_sequences=True, stateful=True)(inputs)\n",
    "    dropout_deception = layers.Dropout(0.3)(lstm_deception)\n",
    "    \n",
    "    # Global pooling to summarize the LSTM output for deception classification\n",
    "    #pooled_output = layers.GlobalAveragePooling1D()(lstm_deception)\n",
    "    #pooled_output = layers.GlobalAveragePooling1D()(lstm_deception, mask=lstm_deception._keras_mask)\n",
    "    \n",
    "    # **Concatenate the action outputs with the LSTM outputs for deception classification**\n",
    "    concat_deception_input = layers.Concatenate()([dropout_deception, action_output])\n",
    "\n",
    "    # Further process the combined information for deception classification\n",
    "    deception_dense = layers.Dense(64, activation='relu')(concat_deception_input)\n",
    "    dropout_combined = layers.Dropout(0.3)(deception_dense)\n",
    "    \n",
    "    # Global pooling to summarize the LSTM and action output combined for deception classification\n",
    "    pooled_output = layers.GlobalAveragePooling1D()(dropout_combined)\n",
    "    \n",
    "    # Deception classification: A binary classification head\n",
    "    deception_output = layers.Dense(DECEPTION_CLASSES, activation='sigmoid', name=\"deception_output\")(pooled_output)\n",
    "\n",
    "    \n",
    "    # Create the model with two outputs\n",
    "    model = models.Model(inputs=inputs, outputs=[action_output, deception_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#Build model\n",
    "model = build_lstm_model(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d60b13d8-fffb-48f6-b4a2-c2f59f452d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_videos(video_list):\n",
    "    videos = []\n",
    "    for v in video_list:\n",
    "        print(v)\n",
    "        frames = load_every_nth_frame(v)\n",
    "        videos.append(frames)\n",
    "    return videos\n",
    "\n",
    "#X_train = load_videos(video_train)\n",
    "#X_test = load_videos(video_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "17b237fe-95ae-4def-b3a8-d970ff2dacea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_videos_and_save(video_paths, action_labels, deception_labels, batch_size=10, save_dir='preprocessed_videos', is_train=True):\n",
    "    \"\"\"\n",
    "    Preprocess videos using load_videos, preprocess_frames_with_vit, and pad_videos,\n",
    "    then save preprocessed videos along with action and deception labels to disk.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Step 1: Process videos in batches\n",
    "    for i in range(0, len(video_paths), batch_size):\n",
    "        batch_files = video_paths[i:i + batch_size]  # Get batch of video paths\n",
    "        batch_action_labels = action_labels[i:i + batch_size]  # Get batch of action labels\n",
    "        batch_deception_labels = deception_labels[i:i + batch_size]  # Get batch of deception labels\n",
    "\n",
    "        # Step 2: Use your existing load_videos function to load this batch\n",
    "        videos = load_videos(batch_files)  # This returns a list of videos (each as a list of frames)\n",
    "\n",
    "        # Step 3: Preprocess and pad the loaded videos using your existing functions\n",
    "        preprocessed_batch = []\n",
    "        for video in videos:\n",
    "            preprocessed_frames = preprocess_frames_with_vit(video)  # Your custom preprocessing function\n",
    "            preprocessed_batch.append(preprocessed_frames)\n",
    "\n",
    "        # Step 4: Save preprocessed videos and corresponding action and deception labels to disk\n",
    "        for idx, video_file in enumerate(batch_files):\n",
    "            video_name = os.path.basename(video_file).split('.')[0]  # Extract the video file name\n",
    "            \n",
    "            # Save the preprocessed frames to disk as a .npy file\n",
    "            np.save(os.path.join(save_dir, f'{video_name}_preprocessed.npy'), preprocessed_batch[idx])\n",
    "            \n",
    "            # Save the corresponding action label for this video\n",
    "            np.save(os.path.join(save_dir, f'{video_name}_action_label.npy'), batch_action_labels[idx])\n",
    "            \n",
    "            # Save the corresponding deception label for this video\n",
    "            np.save(os.path.join(save_dir, f'{video_name}_deception_label.npy'), batch_deception_labels[idx])\n",
    "        \n",
    "        print(f\"{'Training' if is_train else 'Testing'} batch {i // batch_size + 1}/{len(video_paths) // batch_size + 1} processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577eaf0-46b1-443f-acb6-00ee7e9279e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and save training videos along with both action and deception labels\n",
    "preprocess_videos_and_save(video_train, action_train, deception_train, batch_size=BATCH_SIZE, save_dir='preprocessed_train', is_train=True)\n",
    "\n",
    "# Preprocess and save testing videos along with both action and deception labels\n",
    "preprocess_videos_and_save(video_test, action_test, deception_test, batch_size=BATCH_SIZE, save_dir='preprocessed_test', is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5486bcb0-6358-4810-a0bc-5df7bf262ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.load('preprocessed_train/trial_lie_060_deception_label.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76f6b8-abfa-4630-953a-98d9e136ce24",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up logging to track skipped files\n",
    "logging.basicConfig(filename='skipped_files.log', level=logging.WARNING)\n",
    "\n",
    "class VideoDataSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_paths, batch_size, save_dir):\n",
    "        \"\"\"\n",
    "        Custom data sequence to load preprocessed video data, action labels, and deception labels\n",
    "        \"\"\"\n",
    "        self.video_paths = video_paths  # List of video file paths\n",
    "        self.batch_size = batch_size    # Batch size\n",
    "        self.save_dir = save_dir        # Directory where preprocessed data is stored\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.ceil(len(self.video_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one batch of data for training\n",
    "        \"\"\"\n",
    "        #print(f\"Fetching batch {index} for epoch\")  # Debug print\n",
    "        # Get the batch of video file paths\n",
    "        batch_files = self.video_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Load the corresponding batch of preprocessed videos and labels\n",
    "        batch_frames, batch_action_labels, batch_deception_labels = self.load_preprocessed_batch(batch_files)\n",
    "        #print(f\"Loaded {len(batch_frames)} frames for batch {index}\")  # Debug print\n",
    "        \n",
    "        # Ensure they are NumPy arrays\n",
    "        batch_frames = np.array(batch_frames)\n",
    "        batch_action_labels = np.array(batch_action_labels)\n",
    "        batch_deception_labels = np.array(batch_deception_labels)\n",
    "        \n",
    "        # Ensure that labels are structured correctly (tuple format)\n",
    "        return batch_frames, (batch_action_labels, batch_deception_labels)\n",
    "    \n",
    "    def load_preprocessed_batch(self, batch_files):\n",
    "        \"\"\"\n",
    "        Load preprocessed videos, action labels, and deception labels from disk for a given batch\n",
    "        \"\"\"\n",
    "        batch_frames = []\n",
    "        batch_action_labels = []\n",
    "        batch_deception_labels = []\n",
    "        \n",
    "        for video_file in batch_files:\n",
    "            video_name = os.path.basename(video_file).split('.')[0]\n",
    "            \n",
    "            try:\n",
    "                # Load the preprocessed video frames from disk\n",
    "                frames = np.load(os.path.join(self.save_dir, f\"{video_name}_preprocessed.npy\"))\n",
    "                \n",
    "                # Ensure the frame count meets the required PAD_LENGTH\n",
    "                if len(frames) < PAD_LENGTH:\n",
    "                    raise ValueError(f\"Video '{video_file}' has insufficient frames: {len(frames)} (expected {PAD_LENGTH})\")\n",
    "                \n",
    "                # Load the action and deception labels\n",
    "                action_labels = np.load(os.path.join(self.save_dir, f\"{video_name}_action_label.npy\"))\n",
    "                deception_labels = np.load(os.path.join(self.save_dir, f\"{video_name}_deception_label.npy\"))\n",
    "                \n",
    "                # Append data to the batch lists\n",
    "                batch_frames.append(frames)\n",
    "                batch_action_labels.append(action_labels)\n",
    "                batch_deception_labels.append(deception_labels)\n",
    "    \n",
    "            except Exception as e:\n",
    "                # Log the file name and the issue encountered\n",
    "                logging.warning(f\"Skipping file {video_file}: {str(e)}\")\n",
    "                print(f\"Skipping file {video_file} due to error: {str(e)}\")\n",
    "                continue  # Skip to the next video in the batch\n",
    "        \n",
    "        return np.array(batch_frames), np.array(batch_action_labels), np.array(batch_deception_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Shuffle the data after each epoch if necessary\n",
    "        \"\"\"\n",
    "        print(\"Resetting indices and shuffling data for the next epoch\")\n",
    "        # Shuffle if necessary\n",
    "        p = np.random.permutation(len(self.video_paths))\n",
    "        self.video_paths = np.array(self.video_paths)[p]\n",
    "        \n",
    "\n",
    "    def _output_signature(self):\n",
    "        \"\"\"\n",
    "        Define the output signature for tf.data compatibility\n",
    "        \"\"\"\n",
    "        video_shape = (BATCH_SIZE, PAD_LENGTH, embedding_dim)  # Example shape of video frames\n",
    "        action_label_shape = (BATCH_SIZE, PAD_LENGTH, ACTION_CLASSES)  # Example shape for action labels\n",
    "        deception_label_shape = (BATCH_SIZE,)  # Example shape for deception labels\n",
    "        \n",
    "        return (\n",
    "            tf.TensorSpec(shape=video_shape, dtype=tf.float32),\n",
    "            (\n",
    "                tf.TensorSpec(shape=action_label_shape, dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=deception_label_shape, dtype=tf.float32),\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b59e872c-f5b0-497a-bb24-8692e93851e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging to track skipped files\n",
    "logging.basicConfig(filename='skipped_files.log', level=logging.WARNING)\n",
    "\n",
    "class VideoDataSequence(tf.keras.utils.Sequence):\n",
    "    def __init__(self, video_paths, batch_size, save_dir, modelD, total_epochs):\n",
    "        \"\"\"\n",
    "        Custom data sequence to load preprocessed video data, action labels, and deception labels.\n",
    "        This version yields one frame at a time to the model.\n",
    "        \"\"\"\n",
    "        self.video_paths = video_paths  # List of video file paths\n",
    "        self.batch_size = batch_size    # Batch size (though here it’s per video)\n",
    "        self.save_dir = save_dir        # Directory where preprocessed data is stored\n",
    "        self.frames = []\n",
    "        self.action_labels = []\n",
    "        self.deception_labels = []\n",
    "        self.current_video_index = 0  # To keep track of which video is being processed\n",
    "        self.model = modelD             # Stateful model\n",
    "        self.total_epochs = total_epochs  # Total number of epochs\n",
    "        self.current_epoch = 0  # Track the current epoch\n",
    "        self.load_next_video()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of frames across all videos.\n",
    "        Each frame is treated as one batch item.\n",
    "        \"\"\"\n",
    "        total_frames = sum([self.load_frame_count(v) for v in self.video_paths])\n",
    "        return total_frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Yield one frame and its corresponding label at a time.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the next video if we reach the end of the current video's frames\n",
    "            #print(\"\\n\"+str(self.current_video_index))\n",
    "            if len(self.frames) == 0:\n",
    "                self.load_next_video()\n",
    "    \n",
    "            # If no more frames are available, stop yielding (don't raise an exception)\n",
    "            if len(self.frames) == 0:\n",
    "                # Check if we are on the final epoch\n",
    "                if self.current_epoch >= self.total_epochs - 1:\n",
    "                    print(\"Final epoch, no more frames available. Raising IndexError.\")\n",
    "                    raise IndexError(\"No more frames available. Dataset is exhausted.\")\n",
    "                else:\n",
    "                    # Just return an empty batch if it's not the final epoch\n",
    "                    print(\"EMPTY\")\n",
    "                    return np.zeros((1, 1, embedding_dim)), (np.zeros((1, 1, ACTION_CLASSES)), np.zeros((1,1)))\n",
    "    \n",
    "    \n",
    "            #print(\"\\n\" + str(len(self.frames)))\n",
    "    \n",
    "            # Get the current frame, action label, and deception label\n",
    "            frame=self.frames[0]\n",
    "            pop_frame = self.frames.pop(0)\n",
    "            action_label = self.action_labels\n",
    "            deception_label = self.deception_labels\n",
    "            \n",
    "            # Add two extra dimensions to frame\n",
    "            frame = np.reshape(frame, (1, 1) + frame.shape)\n",
    "    \n",
    "            return frame, (np.expand_dims(np.array([action_label]), axis=0), np.expand_dims(np.array([deception_label]), axis=0))\n",
    "\n",
    "        except IndexError as e:\n",
    "            raise e\n",
    "            # When no more frames/videos are available, gracefully stop the generator\n",
    "            #raise IndexError(\"No more frames available. Dataset is exhausted.\")\n",
    "\n",
    "\n",
    "    def load_next_video(self):\n",
    "        \"\"\"\n",
    "        Load the next video into memory, along with its labels.\n",
    "        \"\"\"\n",
    "        #print(\"AAAAAA\")\n",
    "        if self.current_video_index >= len(self.video_paths):\n",
    "            #raise StopIteration(\"No more videos to load.\")  # Signal the end of the generator\n",
    "            self.frames, self.action_labels, self.deception_labels = [], [], []\n",
    "            return  # Do not raise StopIteration here, just return to stop loading further\n",
    "\n",
    "\n",
    "        # Reset the states of all stateful LSTM layers\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, tf.keras.layers.LSTM) and layer.stateful:\n",
    "                layer.reset_states()\n",
    "                \n",
    "        video_file = self.video_paths[self.current_video_index]\n",
    "        video_name = os.path.basename(video_file).split('.')[0]\n",
    "        \n",
    "        try:\n",
    "            # Load the preprocessed video frames from disk\n",
    "            self.frames = list(np.load(os.path.join(self.save_dir, f\"{video_name}_preprocessed.npy\")))            \n",
    "            # Load the action and deception labels\n",
    "            self.action_labels = np.load(os.path.join(self.save_dir, f\"{video_name}_action_label.npy\"))\n",
    "            self.deception_labels = np.load(os.path.join(self.save_dir, f\"{video_name}_deception_label.npy\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Skipping file {video_file}: {str(e)}\")\n",
    "            print(f\"Skipping file {video_file} due to error: {str(e)}\")\n",
    "            self.frames, self.action_labels, self.deception_labels = [], [], []\n",
    "        \n",
    "        self.current_video_index += 1\n",
    "\n",
    "    def load_frame_count(self, video_file):\n",
    "        \"\"\"\n",
    "        Helper function to return the number of frames in a video file.\n",
    "        \"\"\"\n",
    "        video_name = os.path.basename(video_file).split('.')[0]\n",
    "        try:\n",
    "            frames = np.load(os.path.join(self.save_dir, f\"{video_name}_preprocessed.npy\"))\n",
    "            return len(frames)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Skipping file {video_file}: {str(e)}\")\n",
    "            return 0\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Reset the video index and states at the end of each epoch.\n",
    "        \"\"\"\n",
    "        # Increment the current epoch count\n",
    "\n",
    "        # Skip this method if it's the final epoch\n",
    "        self.current_epoch += 1\n",
    "        if self.current_epoch < self.total_epochs:\n",
    "            self.current_video_index = 0\n",
    "            print(\"\\nEPOCH HAS ENDED\" + str(self.current_epoch))\n",
    "        # Shuffle if necessary\n",
    "        #p = np.random.permutation(len(self.video_paths))\n",
    "        #self.video_paths = np.array(self.video_paths)[p]\n",
    "            self.frames=[]\n",
    "            #self.load_next_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6c1a3336-3833-4dba-95d3-08024afa4359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,808</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">213,248</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ action_output       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,709</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ deception_output    │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m98,432\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m263,808\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │    \u001b[38;5;34m213,248\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ action_output       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m21\u001b[0m)        │      \u001b[38;5;34m2,709\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ deception_output    │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">578,262</span> (2.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m578,262\u001b[0m (2.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">578,262</span> (2.21 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m578,262\u001b[0m (2.21 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m173/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - action_output_accuracy: 1.0000 - action_output_loss: 0.0028 - deception_output_accuracy: 1.0000 - deception_output_loss: 1.1921e-07 - loss: 0.0028EMPTY\n",
      "\u001b[1m174/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - action_output_accuracy: 1.0000 - action_output_loss: 0.0028 - deception_output_accuracy: 1.0000 - deception_output_loss: 1.1921e-07 - loss: 0.0028\n",
      "EPOCH HAS ENDED1\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - action_output_accuracy: 1.0000 - action_output_loss: 0.0029 - deception_output_accuracy: 1.0000 - deception_output_loss: 1.1921e-07 - loss: 0.0029EMPTY\n",
      "\n",
      "EPOCH HAS ENDED1\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 176ms/step - action_output_accuracy: 1.0000 - action_output_loss: 0.0029 - deception_output_accuracy: 1.0000 - deception_output_loss: 1.1921e-07 - loss: 0.0029 - val_action_output_accuracy: 0.0000e+00 - val_action_output_loss: 40.9286 - val_deception_output_accuracy: 1.0000 - val_deception_output_loss: 1.1921e-07 - val_loss: 40.9286\n",
      "Epoch 2/3\n",
      "\u001b[1m  1/175\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 148ms/step - action_output_accuracy: 0.0000e+00 - action_output_loss: 0.2511 - deception_output_accuracy: 1.0000 - deception_output_loss: 0.5240 - loss: 0.7750\n",
      "EPOCH HAS ENDED2\n",
      "\u001b[1m  2/175\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 152ms/step - action_output_accuracy: 0.2500 - action_output_loss: 0.1883 - deception_output_accuracy: 1.0000 - deception_output_loss: 0.3930 - loss: 0.5813    \n",
      "EPOCH HAS ENDED2\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - action_output_accuracy: 0.4971 - action_output_loss: 0.1262 - deception_output_accuracy: 1.0000 - deception_output_loss: 0.2635 - loss: 0.3897 - val_action_output_accuracy: 0.0000e+00 - val_action_output_loss: 28.9902 - val_deception_output_accuracy: 1.0000 - val_deception_output_loss: 0.2547 - val_loss: 29.2450\n",
      "Epoch 3/3\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 184ms/step - action_output_accuracy: 0.9858 - action_output_loss: 0.0228 - deception_output_accuracy: 1.0000 - deception_output_loss: 1.1980e-07 - loss: 0.0228 - val_action_output_accuracy: 0.0000e+00 - val_action_output_loss: 46.0084 - val_deception_output_accuracy: 1.0000 - val_deception_output_loss: 1.1921e-07 - val_loss: 46.0084\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(#optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "              optimizer='Adam',\n",
    "              loss={'action_output': 'binary_crossentropy', \n",
    "                    'deception_output': 'binary_crossentropy'},\n",
    "              metrics={'action_output': 'accuracy', 'deception_output': 'accuracy'})\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n",
    "\n",
    "train = []\n",
    "train.append(video_train[0])\n",
    "train.append(video_train[1])\n",
    "\n",
    "test = []\n",
    "test.append(video_test[0])\n",
    "test.append(video_test[1])\n",
    "train_dataset = VideoDataSequence(train, batch_size=BATCH_SIZE, save_dir='preprocessed_train', modelD=model, total_epochs=EPOCHS)\n",
    "test_dataset = VideoDataSequence(test, batch_size=BATCH_SIZE, save_dir='preprocessed_test', modelD=model, total_epochs=EPOCHS)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_dataset,\n",
    "    steps_per_epoch=len(train_dataset)-2,\n",
    "    validation_steps=len(test_dataset)-2,\n",
    "    shuffle=True,  # Disable multiprocessing to ensure no prefetching\n",
    "    verbose=1  \n",
    ")\n",
    "\n",
    "model.save('deception_model.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "7b6a93dc-d1b2-489e-9f0c-8b43f28cd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_data_generator(video_paths, save_dir, model, total_epochs):\n",
    "    \"\"\"\n",
    "    A generator that yields video frames and their corresponding labels for each video.\n",
    "    \"\"\"\n",
    "    current_video_index = 0\n",
    "    current_epoch = 0\n",
    "    while current_epoch < total_epochs:\n",
    "        if current_video_index == 0:\n",
    "            np.random.shuffle(video_paths)  # Shuffle for each new epoch\n",
    "            \n",
    "        if current_video_index >= len(video_paths):\n",
    "            current_epoch += 1\n",
    "            current_video_index = 0\n",
    "            np.random.shuffle(video_paths)  # Optionally shuffle for the next epoch\n",
    "            if current_epoch >= total_epochs:\n",
    "                break\n",
    "        \n",
    "        video_file = video_paths[current_video_index]\n",
    "        video_name = os.path.basename(video_file).split('.')[0]\n",
    "\n",
    "        try:\n",
    "            frames = np.load(os.path.join(save_dir, f\"{video_name}_preprocessed.npy\"))\n",
    "            #print(\"\\n\" + video_name + \" \"+ str(len(frames)))\n",
    "            action_labels = np.load(os.path.join(save_dir, f\"{video_name}_action_label.npy\"))\n",
    "            deception_labels = np.load(os.path.join(save_dir, f\"{video_name}_deception_label.npy\"))\n",
    "\n",
    "            # Reset LSTM states for each new video\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, tf.keras.layers.LSTM) and layer.stateful:\n",
    "                    layer.reset_states()\n",
    "\n",
    "            # for frame in frames:\n",
    "            #     # Yield one frame and its labels at a time\n",
    "            #     frame = np.expand_dims(frame, axis=(0, 1))  # Expands to (1, 1, 768)\n",
    "    \n",
    "            #     yield frame, (np.expand_dims(np.array([action_labels]), axis=0), np.expand_dims(np.array([deception_labels]), axis=0))\n",
    "                        # Yield each frame and its corresponding labels\n",
    "            for i, frame in enumerate(frames):\n",
    "                frame = np.expand_dims(frame, axis=(0, 1))  # Shape becomes (1, 1, 768)\n",
    "                \n",
    "                # Log each frame to ensure all frames are processed\n",
    "                #print(f\"Yielding frame {i + 1}/{len(frames)} from video {video_name}\")\n",
    "    \n",
    "                yield frame, (np.expand_dims(np.array([action_labels]), axis=0), np.expand_dims(np.array([deception_labels]), axis=0))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Skipping file {video_file}: {str(e)}\")\n",
    "            print(f\"Skipping file {video_file} due to error: {str(e)}\")\n",
    "\n",
    "        current_video_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d4e33119-cfd3-4884-ba5c-56f6c8190528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(video_paths, batch_size, save_dir, model, total_epochs, embedding_dim, action_classes):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: video_data_generator(video_paths, save_dir, model, total_epochs),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(1, 1, embedding_dim), dtype=tf.float32),  # Frame\n",
    "            (\n",
    "                tf.TensorSpec(shape=(1, 1, ACTION_CLASSES), dtype=tf.float32),  # Action label\n",
    "                tf.TensorSpec(shape=(1, 1), dtype=tf.float32)  # Deception label\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Shuffle, batch, and prefetch the dataset\n",
    "    #dataset = dataset.shuffle(buffer_size=len(video_paths) * 1)  # Adjust buffer size if needed\n",
    "    #dataset = dataset.batch(batch_size)\n",
    "    #dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    #dataset = dataset.prefetch(1)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "95f90c09-d265-4aa5-90a4-57eb37c03cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9936/9936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1503s\u001b[0m 151ms/step - action_output_accuracy: 0.4423 - action_output_loss: 0.1569 - deception_output_accuracy: 0.9880 - deception_output_loss: 0.0575 - loss: 0.2144 - val_action_output_accuracy: 0.0000e+00 - val_action_output_loss: 2.0269 - val_deception_output_accuracy: 0.9902 - val_deception_output_loss: 0.1313 - val_loss: 2.1582\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(#optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "              optimizer='Adam',\n",
    "              loss={'action_output': 'binary_crossentropy', \n",
    "                    'deception_output': 'binary_crossentropy'},\n",
    "              metrics={'action_output': 'accuracy', 'deception_output': 'accuracy'})\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "train = []\n",
    "train.append(video_train[0])\n",
    "train.append(video_train[1])\n",
    "\n",
    "test = []\n",
    "test.append(video_test[0])\n",
    "test.append(video_test[1])\n",
    "\n",
    "steps_per_epoch = sum([np.load(os.path.join('preprocessed_train', f\"{os.path.basename(video_file).split('.')[0]}_preprocessed.npy\")).shape[0] for video_file in video_train])\n",
    "validation_steps = sum([np.load(os.path.join('preprocessed_test', f\"{os.path.basename(video_file).split('.')[0]}_preprocessed.npy\")).shape[0] for video_file in video_test])\n",
    "\n",
    "train_dataset = create_tf_dataset(\n",
    "    video_paths=video_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    save_dir='preprocessed_train',\n",
    "    model=model,  # Stateful LSTM model\n",
    "    total_epochs=EPOCHS,\n",
    "    embedding_dim=embedding_dim,\n",
    "    action_classes=ACTION_CLASSES\n",
    ")\n",
    "\n",
    "validation_dataset = create_tf_dataset(\n",
    "    video_paths=video_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    save_dir='preprocessed_test',\n",
    "    model=model,  # Stateful LSTM model\n",
    "    total_epochs=EPOCHS,\n",
    "    embedding_dim=embedding_dim,\n",
    "    action_classes=ACTION_CLASSES\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch-1,\n",
    "    validation_steps=validation_steps-1,\n",
    "    validation_data=validation_dataset\n",
    ")\n",
    "\n",
    "model.save('deception_model_generator.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2af6e-825e-4d9f-8837-77b91462147e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action_predictions = model.predict(create_tf_dataset(video_test, batch_size=BATCH_SIZE, save_dir='preprocessed_test', model=model, total_epochs=1, embedding_dim=embedding_dim, action_classes=ACTION_CLASSES))  # Predicts probabilities between 0 and 1\n",
    "\n",
    "#print(predictions)\n",
    "\n",
    "# Threshold for detecting an action\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Iterate over each video's predictions\n",
    "for i, video_pred in enumerate(action_predictions[0]):  # action_predictions[0] corresponds to action output\n",
    "    print(f\"\\nVideo {i+1}:\")\n",
    "    \n",
    "    # Average the predictions over all frames in the video (optional, depends on your approach)\n",
    "    avg_action_pred = np.mean(video_pred, axis=0)\n",
    "    \n",
    "    # Apply the threshold to determine detected actions\n",
    "    detected_actions = avg_action_pred > THRESHOLD\n",
    "    \n",
    "    # Print the detected action classes\n",
    "    detected_action_names = [ACTION_COLUMNS[j] for j, detected in enumerate(detected_actions) if detected]\n",
    "    \n",
    "    if detected_action_names:\n",
    "        print(\"Detected Actions:\", detected_action_names)\n",
    "    else:\n",
    "        print(\"No actions detected above threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89ceb653-4bc7-41f8-9b1f-799984f0274f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_live_video(model, video_source=0):\n",
    "    \"\"\"Predict actions and deception from a live video feed (webcam or camera).\"\"\"\n",
    "    cap = cv2.VideoCapture(video_source)  # 0 is the default webcam\n",
    "    frame_buffer = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video. Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Add the frame to the buffer\n",
    "        frame_resized = cv2.resize(frame, FRAME_SIZE)\n",
    "        frame_buffer=[]\n",
    "        frame_buffer.append(frame_resized)\n",
    "        frame_count += 1\n",
    "\n",
    "        # If we have enough frames (NUM_FRAMES), make predictions\n",
    "        if frame_count % 16 == 0:\n",
    "            # Preprocess frames\n",
    "            \n",
    "            preprocessed_frames = preprocess_frames_with_vit(frame_buffer)\n",
    "            preprocessed_frames = model.predict(np.expand_dims(preprocessed_frames, axis=0))\n",
    "            #print(preprocessed_frames[0])\n",
    "\n",
    "            # Make predictions\n",
    "            action_pred, deception_pred = preprocessed_frames\n",
    "\n",
    "            # Decode predictions\n",
    "            # Apply the threshold to determine detected actions\n",
    "            detected_actions = action_pred[0][0] > 0.5\n",
    "            # Print the detected action classes\n",
    "            detected_action_names = [ACTION_COLUMNS[j] for j, detected in enumerate(detected_actions) if detected]\n",
    "            \n",
    "            if detected_action_names:\n",
    "                print(\"Detected Actions:\", detected_action_names)\n",
    "            else:\n",
    "                print(\"No actions detected above threshold.\")\n",
    "\n",
    "            \n",
    "            # predicted_actions = np.argmax(action_pred, axis=-1)[0]\n",
    "            deception = 'Deceptive' if deception_pred[0][0] > 0.5 else 'Truthful'\n",
    "\n",
    "            # Display predictions\n",
    "            print(f\"Predicted deception: {deception}\")\n",
    "\n",
    "        # Display the live video feed\n",
    "        cv2.imshow('Live Video Feed', frame)\n",
    "\n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efbe6798-4ade-4ac2-8e91-d0c8afb0418e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n",
      "Shape of frame embeddings: (1, 768)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Detected Actions: ['holdingArms', 'soothingArms', 'interlockedHand', 'soothinHandM']\n",
      "Predicted deception: Deceptive\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeception_model_generator.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Perform real-time prediction \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m predict_live_video(model, video_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 8\u001b[0m, in \u001b[0;36mpredict_live_video\u001b[1;34m(model, video_source)\u001b[0m\n\u001b[0;32m      5\u001b[0m frame_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to capture video. Exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = load_model('deception_model_generator.keras')\n",
    "\n",
    "# Perform real-time prediction \n",
    "predict_live_video(model, video_source=0)  # Set to 0 for default webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019ec5d-7164-426d-8e4f-849b84aa4549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
