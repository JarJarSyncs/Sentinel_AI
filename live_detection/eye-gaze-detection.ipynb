{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_face import (\n",
    "    RetinaFace,\n",
    "    SixDRep\n",
    ")\n",
    "from sixdrepnet.model import SixDRepNet\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from math import cos, sin\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sixdrepnet import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transformations\n",
    "transformations = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = RetinaFace(gpu_id=0) # -1 for mac \n",
    "cam = 1\n",
    "device = torch.device('cuda') #-1 for mac\n",
    "model = SixDRepNet(backbone_name='RepVGG-B1g2',\n",
    "                   backbone_file='',\n",
    "                   deploy=True,\n",
    "                   pretrained=False)\n",
    "model.to(device)\n",
    "bw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(image, offset_coeff=1) -> dict:\n",
    "    try:\n",
    "        coeff = 1280 / image.shape[1]\n",
    "        resized_image = cv2.resize(image, (1280, int(image.shape[0]*coeff)))\n",
    "        with torch.no_grad():\n",
    "            faces = detector(resized_image)\n",
    "            result = []\n",
    "            for box, landmarks, score in faces:\n",
    "\n",
    "                # Print the location of each face in this image\n",
    "                if score < .95:\n",
    "                    continue\n",
    "                x_min = int(box[0])\n",
    "                y_min = int(box[1])\n",
    "                x_max = int(box[2])\n",
    "                y_max = int(box[3])\n",
    "\n",
    "                x_min2 = int(box[0])\n",
    "                y_min2 = int(box[1])\n",
    "                x_max2 = int(box[2])\n",
    "                y_max2 = int(box[3])\n",
    "\n",
    "                x_3 = int(landmarks[0][0])\n",
    "                y_3 = int(landmarks[0][1])\n",
    "                x_4 = int(landmarks[1][0])\n",
    "                y_4 = int(landmarks[1][1])\n",
    "\n",
    "                bbox_width = abs(x_max - x_min)\n",
    "                bbox_height = abs(y_max - y_min)\n",
    "\n",
    "                x_min = max(0, x_min-int(0.2*bbox_height))\n",
    "                y_min = max(0, y_min-int(0.2*bbox_width))\n",
    "                x_max += int(0.2*bbox_height)\n",
    "                y_max += int(0.2*bbox_width)\n",
    "                img = resized_image[y_min:y_max, x_min:x_max]\n",
    "                img = Image.fromarray(img)\n",
    "                img = img.convert('RGB')\n",
    "                img = transformations(img)\n",
    "            \n",
    "                img = torch.Tensor(img[None, :]).to(device)\n",
    "                \n",
    "                R_pred = model(img)\n",
    "                \n",
    "                euler = utils.compute_euler_angles_from_rotation_matrices(\n",
    "                    R_pred)*180/np.pi\n",
    "                \n",
    "                curr = {'p_pred_deg': euler[:, 0].cpu(),\n",
    "                        'y_pred_deg': euler[:, 1].cpu(),\n",
    "                        'r_pred_deg': euler[:, 2].cpu()\n",
    "                        }\n",
    "                \n",
    "                offset = abs(((x_3 - x_min2)/2 + (x_max2-x_4)/2)/2)\n",
    "                x_offset = int(offset*1.2*offset_coeff)\n",
    "                y_offset = int(offset*0.8*offset_coeff)\n",
    "\n",
    "                y_3_min = int((y_3 - y_offset) / coeff)\n",
    "                y_3_max = int((y_3 + y_offset) / coeff)\n",
    "                x_3_min = int((x_3 - x_offset) / coeff)\n",
    "                x_3_max = int((x_3 + x_offset) / coeff)\n",
    "\n",
    "                y_4_min = int((y_4 - y_offset) / coeff)\n",
    "                y_4_max = int((y_4 + y_offset) / coeff)\n",
    "                x_4_min = int((x_4 - x_offset) / coeff)\n",
    "                x_4_max = int((x_4 + x_offset) / coeff)\n",
    "\n",
    "                right_eye = image[y_3_min:y_3_max, x_3_min: x_3_max]\n",
    "                left_eye = image[y_4_min:y_4_max, x_4_min: x_4_max]\n",
    "                left_eye = cv2.resize(\n",
    "                    left_eye, (right_eye.shape[1], right_eye.shape[0]))\n",
    "                curr['image'] = cv2.hconcat([right_eye, left_eye])\n",
    "                curr['box'] = list(map(lambda x: x/coeff, box))\n",
    "                curr['landmarks'] = list(\n",
    "                    map(lambda y: list(map(lambda x: x/coeff, y)), landmarks))\n",
    "                result.append(curr)\n",
    "    except Exception as e:\n",
    "        print(e.args)\n",
    "        return None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_eye_axis(img, yaw, pitch, roll, tdx, tdy, size=100):\n",
    "\n",
    "    pitch = pitch * np.pi / 180\n",
    "    yaw = -(yaw * np.pi / 180)\n",
    "    roll = roll * np.pi / 180\n",
    "\n",
    "    x = size * (sin(yaw)) + tdx\n",
    "    y = size * (-cos(yaw) * sin(pitch)) + tdy\n",
    "\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x), int(y)), (255, 255, 0), 3)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a bounding box around the face\n",
    "def draw_face_box(image, box):\n",
    "    x_min, y_min, x_max, y_max = map(int, box)\n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "# Draw landmarks (eyes) on the face\n",
    "def draw_landmarks(image, landmarks):\n",
    "    for (x, y) in landmarks:\n",
    "        cv2.circle(image, (int(x), int(y)), 5, (0, 0, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class SixthEyeNet(nn.ModuleList):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 9, 3)\n",
    "        self.pool = nn.MaxPool2d(3, 3)\n",
    "        self.conv2 = nn.Conv2d(9, 26, 3)\n",
    "        self.fc1 = nn.Linear(3432, 600)\n",
    "        self.fc2 = nn.Linear(600, 50)\n",
    "        self.fc3 = nn.Linear(53, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, head_pos = x\n",
    "        head_pos = head_pos\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.cat((x, head_pos), 1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_axis(img, yaw, pitch, roll, tdx=None, tdy=None, size = 100):\n",
    "\n",
    "    pitch = pitch * np.pi / 180\n",
    "    yaw = -(yaw * np.pi / 180)\n",
    "    roll = roll * np.pi / 180\n",
    "\n",
    "    if tdx != None and tdy != None:\n",
    "        tdx = tdx\n",
    "        tdy = tdy\n",
    "    else:\n",
    "        height, width = img.shape[:2]\n",
    "        tdx = width / 2\n",
    "        tdy = height / 2\n",
    "\n",
    "    # X-Axis pointing to right. drawn in red\n",
    "    x1 = size * (cos(yaw) * cos(roll)) + tdx\n",
    "    y1 = size * (cos(pitch) * sin(roll) + cos(roll) * sin(pitch) * sin(yaw)) + tdy\n",
    "\n",
    "    # Y-Axis | drawn in green\n",
    "    #        v\n",
    "    x2 = size * (-cos(yaw) * sin(roll)) + tdx\n",
    "    y2 = size * (cos(pitch) * cos(roll) - sin(pitch) * sin(yaw) * sin(roll)) + tdy\n",
    "\n",
    "    # Z-Axis (out of the screen) drawn in blue\n",
    "    x3 = size * (sin(yaw)) + tdx\n",
    "    y3 = size * (-cos(yaw) * sin(pitch)) + tdy\n",
    "\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x1),int(y1)),(0,0,255),4)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x2),int(y2)),(0,255,0),4)\n",
    "    cv2.line(img, (int(tdx), int(tdy)), (int(x3),int(y3)),(255,0,0),4)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Spookskiii\\AppData\\Local\\Temp\\ipykernel_9924\\265137252.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(EYE_MODEL_PATH))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-16.36910057067871 0.24464251101016998\n",
      "-18.987245559692383 -6.32611083984375\n",
      "-20.28217887878418 -3.4256279468536377\n",
      "-21.515626907348633 -5.138554096221924\n",
      "-22.703062057495117 -4.861678123474121\n",
      "-22.16594886779785 -0.8218295574188232\n",
      "-20.291980743408203 -2.3192615509033203\n",
      "-22.016714096069336 -1.971238613128662\n",
      "-20.576326370239258 -2.020995855331421\n",
      "-20.601806640625 -4.424383163452148\n",
      "-21.197404861450195 -4.332413673400879\n",
      "-20.94375991821289 -4.069863319396973\n",
      "-20.915847778320312 -2.3117854595184326\n",
      "-20.771196365356445 -1.8115313053131104\n",
      "-20.88547134399414 -0.919060468673706\n",
      "-22.206933975219727 -5.323070526123047\n",
      "-17.966459274291992 -5.73144006729126\n",
      "-18.741769790649414 -4.797057628631592\n",
      "-19.9930362701416 -6.533651828765869\n",
      "-20.5827579498291 -5.151226043701172\n",
      "-16.943622589111328 -10.17187213897705\n",
      "-20.33582878112793 -9.023050308227539\n",
      "-20.312482833862305 -9.656144142150879\n",
      "-20.356544494628906 -7.461590766906738\n",
      "-20.005491256713867 0.1800105720758438\n",
      "-20.145437240600586 -1.5946638584136963\n",
      "-21.23268699645996 0.7665774822235107\n",
      "-22.3024959564209 -2.201023817062378\n",
      "-20.231155395507812 0.5021846294403076\n",
      "-24.331377029418945 -8.025867462158203\n",
      "-24.668649673461914 -5.475968360900879\n",
      "-11.590255737304688 1.4967036247253418\n",
      "-5.00982666015625 7.6343841552734375\n",
      "-2.9638729095458984 4.7748188972473145\n",
      "-12.778850555419922 0.7315411567687988\n",
      "-13.318174362182617 -1.0064828395843506\n",
      "-14.540306091308594 -3.102105140686035\n",
      "-12.362503051757812 -4.073773384094238\n",
      "-11.984468460083008 -3.0099196434020996\n",
      "-12.465282440185547 -1.2053897380828857\n",
      "-12.018760681152344 -0.7063660621643066\n",
      "-14.65044116973877 -1.1984493732452393\n",
      "-9.227802276611328 1.4126520156860352\n",
      "-8.368751525878906 2.6348514556884766\n",
      "-10.89754867553711 3.505394697189331\n",
      "-19.88932228088379 0.5183007717132568\n",
      "-20.379680633544922 1.6761438846588135\n",
      "-19.212535858154297 -0.4304773509502411\n",
      "-18.91668701171875 -2.2294843196868896\n",
      "-17.967206954956055 -2.68713641166687\n",
      "-15.121221542358398 -2.7539188861846924\n",
      "-11.575048446655273 -4.560469627380371\n",
      "-11.526732444763184 -4.456875324249268\n",
      "-12.432032585144043 -3.1619253158569336\n",
      "-14.972543716430664 -3.5134785175323486\n",
      "-13.69253158569336 -4.141908645629883\n",
      "-12.416237831115723 -3.8023829460144043\n",
      "-12.478462219238281 -1.8840270042419434\n",
      "-14.041790962219238 -4.313100337982178\n",
      "-7.870029926300049 -1.6445794105529785\n",
      "-9.857776641845703 -0.7948102951049805\n",
      "-10.908599853515625 -0.3127536475658417\n",
      "-11.099570274353027 -1.655543565750122\n",
      "-11.139939308166504 2.4039924144744873\n",
      "-11.431767463684082 0.6384830474853516\n",
      "-9.907014846801758 9.221495628356934\n",
      "-10.028361320495605 6.939223766326904\n",
      "-8.351667404174805 8.62558650970459\n",
      "-11.594062805175781 11.405234336853027\n",
      "-7.246075630187988 4.788780212402344\n",
      "-5.9095048904418945 7.068487644195557\n",
      "-13.864058494567871 1.7055175304412842\n",
      "-12.778553009033203 1.582547664642334\n",
      "-13.309745788574219 -1.1651663780212402\n",
      "-14.189432144165039 1.010681390762329\n",
      "-11.48229694366455 -0.46548840403556824\n",
      "-10.77127456665039 -0.570319414138794\n",
      "-10.568830490112305 -5.205523490905762\n",
      "-9.524957656860352 -10.347782135009766\n",
      "-10.995824813842773 -10.392333030700684\n",
      "-10.388460159301758 -15.178925514221191\n",
      "-15.743460655212402 -17.045536041259766\n",
      "-15.067771911621094 -18.211116790771484\n",
      "-15.46139144897461 -14.462306022644043\n",
      "-9.760419845581055 -8.031997680664062\n",
      "-1.549215316772461 11.55201244354248\n",
      "-2.479154348373413 7.381977081298828\n",
      "-7.2401933670043945 7.766653537750244\n",
      "-19.464265823364258 -2.8428637981414795\n",
      "-18.532230377197266 -2.71077561378479\n",
      "-18.532230377197266 -2.71077561378479\n",
      "-18.235864639282227 -2.0498006343841553\n",
      "-16.73164176940918 -2.042123556137085\n",
      "-17.098726272583008 10.359118461608887\n",
      "-0.29800498485565186 24.033937454223633\n",
      "-0.6108382344245911 22.36484718322754\n",
      "-1.211956262588501 23.223777770996094\n",
      "-0.4149055480957031 22.069345474243164\n",
      "-5.842098236083984 7.0615410804748535\n",
      "-8.285319328308105 -12.27322769165039\n",
      "-16.408029556274414 0.6691677570343018\n",
      "-16.334535598754883 0.038108132779598236\n",
      "-14.37973403930664 -0.1710362285375595\n",
      "-15.149660110473633 -0.0831277146935463\n",
      "-10.405537605285645 2.2166736125946045\n",
      "-11.788776397705078 2.6367509365081787\n",
      "-15.523868560791016 2.1934969425201416\n",
      "-14.245964050292969 6.769176483154297\n",
      "-10.134047508239746 10.18331527709961\n",
      "-9.082661628723145 1.6641781330108643\n",
      "-14.827266693115234 -4.4071807861328125\n",
      "-27.33979606628418 -8.706358909606934\n",
      "-27.335168838500977 -12.375113487243652\n",
      "-26.57077407836914 -10.532218933105469\n",
      "-24.82451629638672 -8.54192066192627\n",
      "-26.802778244018555 -8.720711708068848\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    transforms = transforms.Compose([transforms.ToPILImage(),\n",
    "                                     transforms.Resize((70, 210)),\n",
    "                                     transforms.ToTensor()])\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    # Check if the webcam is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    net = SixthEyeNet()\n",
    "    EYE_MODEL_PATH = '../eye-gaze-data-loader/models/sixth_eye_net_combined.pth'\n",
    "    bw = False\n",
    "    net.load_state_dict(torch.load(EYE_MODEL_PATH))\n",
    "    net.to(device)\n",
    "    with torch.no_grad():\n",
    "        n = 0\n",
    "        while True:\n",
    "            coeff = 1\n",
    "            _, frame = cap.read()\n",
    "            # images = os.listdir('./datasets/me_test/')\n",
    "            # coeff = 1\n",
    "            # frame = cv2.imread(\n",
    "            #     f'./datasets/me_test/{images[n]}')\n",
    "            input_data = get_input_data(frame)\n",
    "            if input_data is None:\n",
    "                continue\n",
    "            if len(input_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            for face in input_data:\n",
    "                box = face['box']\n",
    "\n",
    "                # Print the location of each face in this image\n",
    "                x_min = int(box[0])\n",
    "                y_min = int(box[1])\n",
    "                x_max = int(box[2])\n",
    "                y_max = int(box[3])\n",
    "\n",
    "                bbox_width = abs(x_max - x_min)\n",
    "                bbox_height = abs(y_max - y_min)\n",
    "\n",
    "                x_min = max(0, x_min-int(0.2*bbox_height))\n",
    "                y_min = max(0, y_min-int(0.2*bbox_width))\n",
    "                x_max += int(0.2*bbox_height)\n",
    "                y_max += int(0.2*bbox_width)\n",
    "\n",
    "                hp = face['p_pred_deg']\n",
    "                hy = face['y_pred_deg']\n",
    "                hr = face['r_pred_deg']\n",
    "\n",
    "                image = face['image']\n",
    "                image = cv2.resize(image, (210, 70),\n",
    "                                   interpolation=cv2.INTER_CUBIC)\n",
    "                if bw:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                image = transforms(image).to(device)\n",
    "                # Check the devices of the inputs before the forward pass\n",
    "                head_pos = torch.unsqueeze(torch.tensor(\n",
    "                    [float(hp), float(hr), float(hy)], dtype=torch.float32), dim=0).to(device)\n",
    "                image = torch.unsqueeze(image, dim=0).to(device)\n",
    "                res = net((image, head_pos))\n",
    "                res = res.tolist()[0]\n",
    "                pitch = res[0]\n",
    "                yaw = -res[1]\n",
    "                \n",
    "                print(pitch, yaw)\n",
    "\n",
    "                draw_axis(frame, yaw, pitch, hr,\n",
    "                                x_min+int(.5*(x_max-x_min)), y_min+int(.5*(y_max-y_min)), size=130*coeff)\n",
    "\n",
    "            cv2.imshow(\"Demo\", frame)\n",
    "\n",
    "            # Check if 'q' is pressed to exit the loop\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "            n += 1\n",
    "\n",
    "    # Release the camera and close the window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: d:\\Uni\\git\\Sentinel_AI\\live_detection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
