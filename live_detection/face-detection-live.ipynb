{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5fa286-347e-409c-a225-ee0698c65ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from batch_face import RetinaFace, LandmarkPredictor, draw_landmarks, Timer\n",
    "import live_pose_estimator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b3c96-76e4-43ab-b747-2a7b464ece4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(frame, faces):\n",
    "    ### Predict landmarks from given face co-ordinates ###\n",
    "    landmarks = predictor(faces, frame, from_fd=True)\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0b087-9816-4f11-88f1-2933902ac643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks_cv(frame, faces, landmarks):\n",
    "    ### Draw landmarks on faces using CV2 - Possible to draw multiple faces with a For loop, however we are only interested in having one face in the frame ### \n",
    "    frame = draw_landmarks(frame, faces[0][0], landmarks[0])\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e115f-725e-4441-aa6e-6b27f035df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_pose(frame, faces_pose):\n",
    "    head_poses = head_pose_estimator(faces_pose, frame, input_face_type='tuple', update_dict=True)\n",
    "    return head_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd1330-e11f-46e0-8bdb-3e3309af5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_head_pose_cube_cv(frame, faces, pose):\n",
    "    head_pose_estimator.plot_pose_cube(frame, faces[0][0], **pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f7a89-bc2a-4924-8ea5-49852d29de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updated_bbox(landmarks):\n",
    "    ldm_new = landmarks[0]\n",
    "    (x1, y1), (x2, y2) = ldm_new.min(0), ldm_new.max(0)\n",
    "    box_new = np.array([x1, y1, x2, y2])\n",
    "    box_new[:2] -= 10\n",
    "    box_new[2:] += 10\n",
    "    faces = [[box_new, None, None]]\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba68f2c-8590-4610-8ac1-a01bc9eb1af5",
   "metadata": {},
   "source": [
    "Opens camera and passes frames to functions, comment/uncomment functions for desired tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb35c8d-471b-4448-bb59-f17bb6426fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Open camera ###\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = RetinaFace(0)\n",
    "predictor = LandmarkPredictor(0)\n",
    "head_pose_estimator = SixDRep(0)\n",
    "detect_time = time.time()\n",
    "faces = None\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    loop_time = time.time()\n",
    "    \n",
    "    ### NOTE: RGB values are normalized within RetinaFace ###\n",
    "    ### Detect faces if none exist ###\n",
    "    \n",
    "    # Calculate the time difference\n",
    "    elapsed_time = time.time() - detect_time\n",
    "\n",
    "    # Check if n seconds has passed: The shorter the elapsed time - the more face detections are done, but also the lower the fps and efficiency\n",
    "    if faces is None or elapsed_time >= 1:\n",
    "        faces = detector(frame, cv=True, threshold=0.5)\n",
    "        detect_time = time.time()\n",
    "    else:\n",
    "        ### This is an efficiency method of predicting the face bound-box - especially for live camera. It uses the min and max values from the results of the previous landmark 'predictor' function. Helps increase the fps rate ###\n",
    "        ### However, it will not detect new faces, or when a face has gone ###\n",
    "        faces = updated_bbox(landmarks)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        print(\"NO face is detected!\")\n",
    "        continue\n",
    "\n",
    "    ### Predict landmarks from face ###\n",
    "    landmarks = get_landmarks(frame, faces)\n",
    "\n",
    "    ### Estimate head pose from face ###\n",
    "    pose = get_head_pose(frame, faces)\n",
    "    \n",
    "    ### Draw landmarks (AND/OR) pose cube ###\n",
    "    frame = draw_landmarks_cv(frame, faces, landmarks)\n",
    "    draw_head_pose_cube_cv(frame, faces, pose[0])\n",
    "\n",
    "    # Calculate and display FPS, Pitch, Yaw and Roll\n",
    "    fps = 1 / (time.time() - loop_time)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, f\"Pitch: {pose[0]['pitch']:.2f}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Yaw: {pose[0]['yaw']:.2f}\", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Roll: {pose[0]['roll']:.2f}\", (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    ### Display the resulting frame ###\n",
    "    cv2.imshow('', frame)\n",
    "\n",
    "    ### Press 'q' to exit the video window ###\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "### Release the capture when done ###\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481b2eb-baea-4157-b664-8dfdcbdd64f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sixdrepnet import SixDRepNet\n",
    "import sixdrepnet.utils as utils\n",
    "from opencv_transforms import transforms as cv_transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "crop_resize = cv_transforms.Compose([cv_transforms.Resize(224),\n",
    "                                    cv_transforms.CenterCrop(224)])\n",
    "\n",
    "normalize = cv_transforms.Compose([cv_transforms.ToTensor(),\n",
    "                                    cv_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "def chunk_generator(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def chunk_call(model, chunk_size, input_tensor):\n",
    "    outputs = []\n",
    "    for chunk in chunk_generator(input_tensor, chunk_size):\n",
    "        outputs.append(model(chunk))\n",
    "    if isinstance(outputs[0], torch.Tensor):\n",
    "        return torch.cat(outputs, dim=0)\n",
    "    else:\n",
    "        return flatten(outputs)\n",
    "\n",
    "class SixDRep:\n",
    "    def __init__(self, gpu_id: int= -1, dict_path: str='') -> None:\n",
    "        self.model = SixDRepNet(gpu_id=gpu_id, dict_path=dict_path)\n",
    "        if gpu_id == -1:\n",
    "            self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cuda:{}'.format(gpu_id))\n",
    "\n",
    "    def plot_pose_cube(self, frame, box, yaw, pitch, roll):\n",
    "        x_min = int(box[0])\n",
    "        y_min = int(box[1])\n",
    "        x_max = int(box[2])\n",
    "        y_max = int(box[3])\n",
    "        bbox_width = abs(x_max - x_min)\n",
    "        bbox_height = abs(y_max - y_min)\n",
    "\n",
    "        x_min = max(0, x_min-int(0.2*bbox_height))\n",
    "        y_min = max(0, y_min-int(0.2*bbox_width))\n",
    "        x_max = x_max+int(0.2*bbox_height)\n",
    "        y_max = y_max+int(0.2*bbox_width)\n",
    "        utils.plot_pose_cube(frame,  yaw, pitch, roll, x_min + int(.5*(x_max-x_min)), y_min + int(.5*(y_max-y_min)), size=bbox_width)\n",
    "\n",
    "    def __call__(self, all_faces, frames, batch_size=None, input_face_type='tuple', update_dict=True):\n",
    "        '''\n",
    "        frames: list of np.ndarray, 0~255, uint8, rgb order\n",
    "        batch_size: int, if None, no chunking\n",
    "        input_face_type: str, 'tuple' or 'dict' or 'box'\n",
    "        update_dict: bool, if True, update the input dictionary with head pose\n",
    "        '''\n",
    "        # if update_dict:\n",
    "        #     assert input_face_type == 'dict', 'input_face_type should be dict when updating dictionary'\n",
    "\n",
    "        #assert len(frames) == len(all_faces) M\n",
    "        if batch_size is None:\n",
    "            batch_size = len(all_faces) # no chunking\n",
    "        imgs_for_model = []\n",
    "        metas = []\n",
    "        for face, i in zip(all_faces, range(len(frames))):\n",
    "            #for j, face in enumerate(faces): M\n",
    "            frame = frames #M\n",
    "            if input_face_type == 'tuple':\n",
    "                box = face[0]\n",
    "            elif input_face_type == 'dict':\n",
    "                box = face['box']\n",
    "            elif input_face_type == 'box':\n",
    "                box = face\n",
    "            x_min = int(box[0])\n",
    "            y_min = int(box[1])\n",
    "            x_max = int(box[2])\n",
    "            y_max = int(box[3])\n",
    "            \n",
    "            bbox_width = abs(x_max - x_min)\n",
    "            bbox_height = abs(y_max - y_min)\n",
    "\n",
    "            x_min = max(0, x_min-int(0.2*bbox_height))\n",
    "            y_min = max(0, y_min-int(0.2*bbox_width))\n",
    "            x_max = x_max+int(0.2*bbox_height)\n",
    "            y_max = y_max+int(0.2*bbox_width)\n",
    "            img = frame[y_min:y_max, x_min:x_max]\n",
    "            imgs_for_model.append(normalize(crop_resize(img)))\n",
    "            metas.append((i, 0, x_min, y_min, x_max, y_max, bbox_width, bbox_height))\n",
    "\n",
    "                # pitch, yaw, roll = model.predict(img)\n",
    "                # img = model.draw_axis(img, yaw, pitch, roll)\n",
    "                # frame[y_min:y_max, x_min:x_max] = img\n",
    "\n",
    "                # utils.plot_pose_cube(frame,  yaw, pitch, roll, x_min + int(.5*(\n",
    "                #             x_max-x_min)), y_min + int(.5*(y_max-y_min)), size=bbox_width)\n",
    "\n",
    "        imgs_for_model = torch.stack(imgs_for_model).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            pred = chunk_call(self.model.model, batch_size, imgs_for_model)\n",
    "\n",
    "        euler = utils.compute_euler_angles_from_rotation_matrices(pred)*180/np.pi\n",
    "        p = euler[:, 0].cpu().detach().numpy()\n",
    "        y = euler[:, 1].cpu().detach().numpy()\n",
    "        r = euler[:, 2].cpu().detach().numpy()\n",
    "\n",
    "        # reorganize the output\n",
    "        outputs = [] #[] for _ in range(len(frames)) M\n",
    "\n",
    "        for (i, j, x_min, y_min, x_max, y_max, bbox_width, bbox_height), pitch, yaw, roll in zip(metas, p, y, r):\n",
    "            #utils.plot_pose_cube(frames[i], yaw, pitch, roll, x_min + int(.5*(x_max-x_min)), y_min + int(.5*(y_max-y_min)), size=bbox_width)\n",
    "            head_pose = {\n",
    "                'pitch': pitch,\n",
    "                'yaw': yaw,\n",
    "                'roll': roll\n",
    "            }\n",
    "            outputs.append(head_pose)\n",
    "            if update_dict and input_face_type == 'dict':\n",
    "\n",
    "                all_faces[0]['head_pose'] = head_pose\n",
    "        #for faces, output in zip(all_faces, outputs):\n",
    "            #assert len(faces) == len(output)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bdccf-8d98-46e7-81c9-226f052b8186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ddd2c23c291ca41a30efe192477b0967ab2f83f0d470686ada35863dfaa7bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
